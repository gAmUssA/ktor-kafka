ktor {
  kafka {
    # Required connection configs for Kafka producer, consumer, and admin
    bootstrap.servers = ["localhost:29092"]

    properties {
      //      security.protocol = SASL_SSL
      //      sasl.jaas.config = "org.apache.kafka.common.security.plain.PlainLoginModule   required username='userName'   password='password';"
      //      sasl.mechanism = PLAIN
      //      # Required for correctness in Apache Kafka clients prior to 2.6
      //      client.dns.lookup = use_all_dns_ips
      //      # Best practice for Kafka producer to prevent data loss
      //      acks = all

      //# Required connection configs for Confluent Cloud Schema Registry
      schema.registry.url = "http://localhost:8081"
      //      basic.auth.credentials.source = "USER_INFO"
      //      basic.auth.user.info = "{{ SR_API_KEY }}:{{ SR_API_SECRET }}"
    }
    consumer {
      group.id = "ktor-consumer"
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer = io.confluent.kafka.serializers.json.KafkaJsonSchemaDeserializer
      json.value.type = io.confluent.developer.Question
    }
    producer {
      client.id = "ktor-producer"
      key.serializer = org.apache.kafka.common.serialization.LongSerializer
      value.serializer = io.confluent.kafka.serializers.json.KafkaJsonSchemaSerializer
    }
    streams {
      application.id = "ktor-stream"
      # TODO: cloud should be 3
      replication.factor = 1
      //cache.max.size.buffering = 1024
      cache.max.bytes.buffering = 0
      default.topic.replication.factor = 1
      //default.key.serde=
      //default.value.serde
    }
  }
}
